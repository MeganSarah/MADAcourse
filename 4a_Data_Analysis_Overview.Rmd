---
title: MADA Fall 2019 - Data Analysis Overview
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r Sys.Date()`"
#bibliography: ../media/references.bib
output: 
  html_document:
    toc_depth: 3
---

```{r, include=FALSE}
#just so I can use emoticons
#devtools::install_github("hadley/emo")
library(emo)
#set some figure options
knitr::opts_chunk$set(out.width = '70%')
```


# Overview
In this module, we will talk about the whole data analysis workflow in a general, big-picture sense before we go into some of the details in further modules.


# Learning Objectives
*	Understand the elements of a data analysis.  
* Know the role and importance of each component of a data analysis.


# Data Analysis Workflow
The following figure provides a conceptual illustration of the whole data analysis workflow.


```{r workflow,  echo=FALSE, fig.cap='Data analysis workflow', out.width = '60%', fig.align='center'}
knitr::include_graphics("./media/analysis-workflow.png")
```



The spiral is meant to indicate that while the different steps in an analysis are rarely linear, things do improve over time until you have the data and code in a form that everything fits and you can produce _final_ results.

In the introductory chapter of their great book [R for data Science (R4DS)](https://r4ds.had.co.nz/), Garrett Grolemund and Hadley wickham have a similar diagram. [Check out the figure and read their short introductory chapter (1.1).](https://r4ds.had.co.nz/introduction.html)

Note that the process of getting the data ready for analysis goes by different names that people use interchangably. So when you hear or read words like _wrangling_, _cleaning_, _processing_ or _tidying_ it often means the same thing. Sometimes people try to distinguish, e.g. _tidying_ could be used to get the data into something called tidy format (we'll talk about that more later), while _processing_ could mean things like dealing with outliers or missing values. In my figure above, I wrote those two terms as separate, while in the R4DS chapter and figure, they distinguish between tidying and transforming and together call them wrangling. You need to guess from the context what is meant, but often you can think of these terms as all meaning similar things.


Let's look at the different components of an analysis in more detail. The following sections are meant as conceptual overview. Apart from the _Question and Data Match_ discussion, we will get into each of the other steps and how to do them in more detail later in the course.


# Question & Data Match

Maybe the most important challenge of a good scientific project (and thus an interesting and useful data analysis) is to find a combination of questions and data that lead to a project that is both  both _interesting_ and _doable_. One could argue that having a good question and suitable data to answer it is part of science/research but not strictly part of the analysis. However, for your data analyis to be meaningful, it is critical to get the question-data match right. 

**If you start with a boring (or dumb) question or with data that is essentially garbage/noise, or you have a bad question/data match, no part of your analysis matters!**

There are different approaches of finding a question-data match. The classical one is to have a specific question or hypothesis, then design a study to collect data, and analyze the collected data to adress that specific question. 

If you have a question but you do not have the resources to collect data specifically to answer your question, you can look for existing data that might help you answer your question. Rarely will you find data that allows you to exactyl answer your initial question. But you might find data that allows you to answer a very similar question. You might need to iterate a few times by modifying your question and changing the datasets you use before you have a good match, i.e. a good question and data that allows you to answer it.

Another approach that is becoming more common is to start with the data. You might come across one or multiple datasets that interest you. You can then investigate the data to see what questions you might be able to answer with it. Since the data was not collected to answer your specific question, you will have to be somewhat flexible with the question and let the data guide your analysis. This approach can be very powerful since more and more data are becoming available for analysis. The caveat is that if you poke any data hard enough, you will eventually find something _statistically significant_ but you cannot interpret this in the same way as a _statistically significant_ result obtained by pre-specifying the question, collecting data specifically to answer the question and analyzing the data to answer the initial question.

[This website provides a nice example](https://projects.fivethirtyeight.com/p-hacking/) of a setting where you can torture your data by including/excluding different bits to get any kind of correlation you want.


# Getting and loading data
In some sense the best, but also the most money- and time-consuming way of getting data is to collect it yourself. Doing so ensures that the data can answer the question you have. You can also control the quality of the data that is being collected - at least to some extent. However, it is often not feasible to collect your own data. You might not have funding or time or interest to do so. The next best source of data is directly from someone who collected it. The advantage of such data is that you can ask someone for clarification. Also, the person who collected the data is likely a subject matter expert, which can lead to asking better questions. Finally, you can get data that is generally available, e.g. in some online resource. This gives you access to a lot of different datasets. The drawback is that the data was not collected to answer your specific question, and there is usually nobody you can ask for clarification.

No matter the source, you need to get the data into your favorite data analysis system (in our case that will be R). Sometimes, you get data in a format that can be read in easily, e.g. a comma-separated CSV file without any strange formatting that messes up import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets or you get it by scraping data from some online source. We'll look at ways of getting data into R in a later module.  



# Cleaning Data

The amount of data cleaning (also called _tidying_ or _wrangling_) you need to do depends very much on your data. 

A general rule is that you should never directly edit your raw data. If you can, load the raw data into R and do all cleaning inside R with code, so everyting is automatically reproducible and documented. 

Sometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it's close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won't import into R. In those cases, you might have to make modifications in a software other than R. 

If you can't directly read the data into R and need to make some changes before, make copies of your raw data AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made. 

There are certain cleaning actions you will need to do with almost any dataset. You will have to find and deal with data entry errors. If the mistake is obvious, you can fix it. If you know the value is wrong but don't know what it should be, you might need to discard that data. Sometimes you get data that seems way outside the range of all other values, but it is not obviously wrong. For instance if you had a group of individuals and their weights, a person with a weight of 8000 pounds is obviously not possible and a data entry. Someone with a 800 pound weight seems wrong, but [it's not completely impossible](https://en.wikipedia.org/wiki/Jon_Brower_Minnoch). You will have to decide how to deal with such outliers, either during the cleaning or the analysis stage.

In almost any datasets, some values are missing. First, you need to figure out how missing values are coded and what _missing_ means. Sometimes an empty entry means it was not recorded. At other times, it could be that it is not appplicable. For instance on a survey that asks if someone ever smokes, and then as follow-up for how long, if they say they never smoked, the "how long" question is moot and might be empty. For your analysis you might want to code it as duration of 0. We'll discuss in more detail how to deal with missing values later in the course.

As you do all the cleaning, you need to get your data into a shape that is ready for analysis. If you apply more than one analytic method, you might need different clean versions of your dataset.


# Exploratory Data Analysis
To properly clean your data, you will already have to start exploring it. 

* Look at summaries for individual variables
* Slice and dice your data any way you can think of
* Plot everything!
* "Get to know" your data

**Checkpoint:** Hopefully you'll figure out at this stage at the latest if your project is feasible or not, i.e. if you can use the data to answer your question. If not, alter or abandon. Don't keep going hoping against all hope that "magic might happen" and you'll get a good final product after all.


# Pre-process Data

Additional data processing could be considered part of the cleaning process, or you can think of it as a separate step. Depending on the analytic method you use, some further data processing might be suitable. For instance some methods work better if you center and scale your predictors. Some methods, such as linear models and related approaches, are not able to handle missing values, so you need to decide how to deal with those. Sometimes it might be good to not include all variables in your model, or to use the existing variables to create new ones (e.g. instead of including height and weight in your model, you might decide to compute BMI and use that variable in your model instead.) The specific pre-processing steps that need to happen depends on the statistical modeling approach(es) you plan on using.


# Analyzing Data
You finally reached the step where you can apply actual statistical analysis to your data. This topic takes up the majority of time in most (bio)statistics courses. While it is obviously very important to get the statistical analysis right, for any real data analysis you will likely spend <25% of your time on this task, the majority of your time (though not computing time) goes into the other tasks.

At this stage in the process you should now your data well enough to apply statistical methods and approaches that are suitable for your data and question. You can either use a single approach, or explore multiple different ones. As you will see in this class, trying more than one approach is quite easy. If you try multiple models, you need to be careful to not overfit by only picking and reporting the models and analyses that give you the results you want. If you analyze data multiple ways, report all results. 


# Evaluate Results
* Do the results make sense?
* Is the model appropriate for the data? (Use graphs to check)
* Anything that could have gone wrong?

# Reporting your results
* Figures
* Tables
* Manuscript
* Concise but complete
* Supplementary materials allow you to give as much detail as needed
* Reproducibility is important


# Ideal versus real workflow
* Theoretically/ideally, the data analysis workflow is one-directional
* In reality, there are often back and forths.
* Some things that might happen:
    * Running analysis shows that cleaning/pre-processing wasn't adequate
    * Changing analysis method requires different kinds of cleaning and pre-processing.
    * Evaluating results indicates that something 'isn't quite right' and needs to be fixed
    * Multiple variations of the analysis based on better understanding how to best present results

**Having the whole analysis reproducible and well documented is important**


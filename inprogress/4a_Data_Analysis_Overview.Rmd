---
title: MADA Fall 2019 - Data Analysis Overview
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r Sys.Date()`"
#bibliography: ../media/references.bib
output: 
  html_document:
    toc_depth: 3
---

```{r, include=FALSE}
#just so I can use emoticons
#devtools::install_github("hadley/emo")
library(emo)
#set some figure options
knitr::opts_chunk$set(out.width = '70%')
```


# Overview
In this module, we will talk about the whole data analysis workflow in a general, big-picture sense before we go into some of the details in further modules.


# Learning Objectives
*	Understand the elements of a data analysis  
*	Be able to set up a structure for an efficient, reproducible analysis

# Data Analysis Workflow
The following figure provides a conceptual illustration of the whole data analysis workflow.


```{r workflow,  echo=FALSE, fig.cap='Data analysis workflow', out.width = '60%', fig.align='center'}
knitr::include_graphics("./media/analysis-workflow.png")
```



The spiral is meant to indicate that while the different steps in an analysis are rarely linear, things do improve over time until you have the data and code in a form that everything fits and you can produce _final_ results.

In the introductory chapter of their great book [R for data Science (R4DS)](https://r4ds.had.co.nz/), Garrett Grolemund and Hadley wickham have a similar diagram. [Check out the figure and read their short introductory chapter (1.1).](https://r4ds.had.co.nz/introduction.html)

Note that the process of getting the data ready for analysis goes by different names that people use interchangably. So when you hear or read words like _wrangling_, _cleaning_, _processing_ or _tidying_ it often means the same thing. Sometimes people try to distinguish, e.g. _tidying_ could be used to get the data into something called tidy format (we'll talk about that more later), while _processing_ could mean things like dealing with outliers or missing values. In my figure above, I wrote those two terms as separate, while in the R4DS chapter and figure, they distinguish between tidying and transforming and together call them wrangling. You need to guess from the context what is meant, but often you can think of these terms as all meaning similar things.


Let's look at the different components of an analysis in more detail. The following sections are meant as conceptual overview. Apart from the _Question and Data Match_ discussion, we will get into each of the other steps and how to do them in more detail later in the course.


# Question & Data Match

Maybe the most important challenge of a good scientific project (and thus an interesting and useful data analysis) is to find a combination of questions and data that lead to a project that is both  both _interesting_ and _doable_. One could argue that having a good question and suitable data to answer it is part of science/research but not strictly part of the analysis. However, for your data analyis to be meaningful, it is critical to get the question-data match right. 

**If you start with a boring (or dumb) question or with data that is essentially garbage/noise, or you have a bad question/data match, no part of your analysis matters!**

There are different approaches of finding a question-data match. The classical one is to have a specific question or hypothesis, then design a study to collect data, and analyze the collected data to adress that specific question. 

If you have a question but you do not have the resources to collect data specifically to answer your question, you can look for existing data that might help you answer your question. Rarely will you find data that allows you to exactyl answer your initial question. But you might find data that allows you to answer a very similar question. You might need to iterate a few times by modifying your question and changing the datasets you use before you have a good match, i.e. a good question and data that allows you to answer it.

Another approach that is becoming more common is to start with the data. You might come across one or multiple datasets that interest you. You can then investigate the data to see what questions you might be able to answer with it. Since the data was not collected to answer your specific question, you will have to be somewhat flexible with the question and let the data guide your analysis. This approach can be very powerful since more and more data are becoming available for analysis. The caveat is that if you poke any data hard enough, you will eventually find something _statistically significant_ but you cannot interpret this in the same way as a _statistically significant_ result obtained by pre-specifying the question, collecting data specifically to answer the question and analyzing the data to answer the initial question.

[This website provides a nice example](https://projects.fivethirtyeight.com/p-hacking/) of a setting where you can torture your data by including/excluding different bits to get any kind of correlation you want.


# Getting and loading data
In some sense the best, but also the most money- and time-consuming way of getting data is to collect it yourself. Doing so ensures that the data can answer the question you have. You can also control the quality of the data that is being collected - at least to some extent. However, it is often not feasible to collect your own data. You might not have funding or time or interest to do so. The next best source of data is directly from someone who collected it. The advantage of such data is that you can ask someone for clarification. Also, the person who collected the data is likely a subject matter expert, which can lead to asking better questions. Finally, you can get data that is generally available, e.g. in some online resource. This gives you access to a lot of different datasets. The drawback is that the data was not collected to answer your specific question, and there is usually nobody you can ask for clarification.

No matter the source, you need to get the data into your favorite data analysis system (in our case that will be R). Sometimes, you get data in a format that can be read in easily, e.g. a comma-separated CSV file without any strange formatting that messes up import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets or you get it by scraping data from some online source. We'll look at ways of getting data into R in a later module.  



# Cleaning Data

The amount of data cleaning (also called _tidying_ or _wrangling_) you need to do depends very much on your data. 

A general rule is that you should never directly edit your raw data. If you can, load the raw data into R and do all cleaning inside R with code, so everyting is automatically reproducible and documented. 

Sometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it's close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won't import into R. In those cases, you might have to make modifications in a software other than R. 

If you can't directly read the data into R and need to make some changes before, make copies of your raw data AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made. 

There are certain cleaning actions you will need to do with almost any dataset. You will have to find and deal with data entry errors. If the mistake is obvious, you can fix it. If you know the value is wrong but don't know what it should be, you might need to discard that data. Sometimes you get data that seems way outside the range of all other values, but it is not obviously wrong. For instance if you had a group of individuals and their weights, a person with a weight of 8000 pounds is obviously not possible and a data entry. Someone with a 800 pound weight seems wrong, but [it's not completely impossible](https://en.wikipedia.org/wiki/Jon_Brower_Minnoch). You will have to decide how to deal with such outliers, either during the cleaning or the analysis stage.

In almost any datasets, some values are missing. First, you need to figure out how missing values are coded and what _missing_ means. Sometimes an empty entry means it was not recorded. At other times, it could be that it is not appplicable. For instance on a survey that asks if someone ever smokes, and then as follow-up for how long, if they say they never smoked, the "how long" question is moot and might be empty. For your analysis you might want to code it as duration of 0. We'll discuss in more detail how to deal with missing values later in the course.

As you do all the cleaning, you need to get your data into a shape that is ready for analysis. If you apply more than one analytic method, you might need different clean versions of your dataset.


# Exploratory Data Analysis
To properly clean your data, you will already have to start exploring it. 

* Look at summaries for individual variables
* Slice and dice your data any way you can think of
* Plot everything!
* "Get to know" your data

**Checkpoint:** Hopefully you'll figure out at this stage at the latest if your project is feasible or not, i.e. if you can use the data to answer your question. If not, alter or abandon. Don't keep going hoping against all hope that "magic might happen" and you'll get a good final product after all.


# Pre-process Data

Additional data processing could be considered part of the cleaning process, or you can think of it as a separate step. Depending on the analytic method you use, some further data processing might be suitable. For instance some methods work better if you center and scale your predictors. Some methods, such as linear models and related approaches, are not able to handle missing values, so you need to decide how to deal with those. Sometimes it might be good to not include all variables in your model, or to use the existing variables to create new ones (e.g. instead of including height and weight in your model, you might decide to compute BMI and use that variable in your model instead.) The specific pre-processing steps that need to happen depends on the statistical modeling approach(es) you plan on using.


# Analyzing Data
You finally reached the step where you can apply actual statistical analysis to your data. This topic takes up the majority of time in most (bio)statistics courses. While it is obviously very important to get the statistical analysis right, for any real data analysis you will likely spend <25% of your time on this task, the majority of your time (though not computing time) goes into the other tasks.

At this stage in the process you should now your data well enough to apply statistical methods and approaches that are suitable for your data and question. You can either use a single approach, or explore multiple different ones. As you will see in this class, trying more than one approach is quite easy. If you try multiple models, you need to be careful to not overfit by only picking and reporting the models and analyses that give you the results you want. If you analyze data multiple ways, report all results. 


# Evaluate Results
* Do the results make sense?
* Is the model appropriate for the data? (Use graphs to check)
* Anything that could have gone wrong?

# Reporting your results
* Figures
* Tables
* Manuscript
* Concise but complete
* Supplementary materials allow you to give as much detail as needed
* Reproducibility is important


# Ideal versus real workflow
* Theoretically/ideally, the data analysis workflow is one-directional
* In reality, there are often back and forths.
* Some things that might happen:
    * Running analysis shows that cleaning/pre-processing wasn't adequate
    * Changing analysis method requires different kinds of cleaning and pre-processing.
    * Evaluating results indicates that something 'isn't quite right' and needs to be fixed
    * Multiple variations of the analysis based on better understanding how to best present results

**Having the whole analysis reproducible and well documented is important**



# Data Analysis Types
No matter what analysis you do, you will need to do the steps of getting and cleaning/processing/wrangling the data. You also always want to explore your data. After that, the further analyis steps you do depend on your goals.

The simplest analysis is a **descriptive** one. At that stage, you process, summarize and present the data, and do not go further. Here, you don't need to fit any statistical models.

* **Analytical/Inferential:** "Statistical Probing" of patterns. e.g. Hypothesis is that there is a negative correlation between age and measles incidence, or that there is a difference between males and females. We can use the right data and model to test if that's true and how strong the correlation is.
* **Predictive:** Given all kinds of information (e.g. gender, age), try to predict future risk of measles infection.
* **Causal:** Trying to show that ice cream causes measles. For that, data needs to be collected in the right way.
* **Mechanistic:** Build a model that explicitly includes mechanisms of measles infection, e.g. a process by which an infected person transmits to a non-infected person. 


### Needs model fitting:
* **Analytical/Inferential: Quantitatively understand relations between inputs/predictors/features and outputs/outcomes**
    * Causal interpretation possible under certain circumstances
* **Predictive: Use information on inputs to predict outputs**
* Mechanistic: Can be used with the right kind of data and models, both in an inferential and predictive way

## Why fit models
* To test hypotheses (e.g. "there is a (linear) _correlation_ between BMI and diabetes")
* To estimate parameters (e.g. "a 1 unit increase of particulate matter leads to 2 more asthma attacks per person per year")
* To make predictions (e.g. " exposure to N cholera bacteria leads to an infection with probability p") 
* To draw causal conclusions (e.g. "taking statins _causes_ reduction in cholesterol")
* To draw mechanistic conclusions (e.g. "interferon reduces HCV virus load _by stopping production of new virus_") 

## Inference as goal
* Focus is on understanding the relation between inputs/predictors/features and outputs/outcome
* Simpler models (e.g. linear/logistic regression, GLM, Classification and Regression Trees) are usually better, allow for easier interpretation of results
* Examples: 
    * Is the relation between some chemical and cancer risk linear, and how much would risk increase if the chemical exposure increased by 1 unit?
    * Understanding which markers (e.g. cytokines in blood) are most influential for a given clinical outcome

## Prediction as goal
* Focus is on predicting (new) output/outcomes based on knowledge of inputs/predictors/features
* Understanding relation between input(s) and output(s) not that important
* High performance, "black box" models (e.g. GAM, Forests, SVM, Neural Nets) are often used.
* Examples: 
    * Netflix/Amazon recommendations based on past behavior
    * Prediction of clinical outcome based on a large set of markers (e.g. cytokines in blood)


## Types of Analyses based on outcome
* If an outcome is known/available, the statistical methods employed are usually called supervised learning methods/algorithms.
    * Example: Look for associations (make predictions) between flu patient characteristics and occurrence of hospitalization.
    
* If an outcome is unknown/unavailable, the statistical methods are called unsupervised learning methods/algorithms.
    * Example: Try to group pictures of cells into categories of normal/abnormal, without knowing for any of the pictures if they are normal or not.

_(Almost) all models you have encountered so far in your work are likely supervised methods._

## Types of Analyses based on outcome

### Outcome is a continuous variable (or can be treated as one)
Regression methods: (generalized) linear models, generalized additive models, trees, support vector machines, neural nets, ...

Examples: Cholesterol level, number of infected cases during outbreak

### Outcome is categorical
Classification Methods: Logistic regression, K-nearest neighbors, Linear Discriminant Analysis, GAM, trees, SVM, NN,...

Examples: Yes/No outcome, low/medium/high responders

## Types of Analyses based on outcome

### Outcome is unknown/does not exist
Clustering methods: K-means clustering, Principal Component Analysis (also used for other purposes), ...

Example: Group gene sequences based on sequence similarity


## The _no free lunch_ theorem
http://en.wikipedia.org/wiki/No_free_lunch_theorem

* There is no method that is universally best for all data/questions
* Even for a given dataset, there are always trade-offs between methods/approaches 

## Machine learning methods
```{r islrfig,  fig.show='hold', out.width = "600px", fig.cap='From: "An Introduction to Statistical Learning with Applications in R" (ISLR) by James et al.', echo=FALSE}
knitr::include_graphics("../media/islr-fig27.png")
```


## Supervised modeling overview
* We consider models where we have a single outcome, Y
* Y can be continuous or categorical
* We usually have multiple predictors, $X_1, X_2,...$
* Sometimes, one of the predictors is our main focus (exposure), and others are covariates
* Sometimes, all predictors are considered equally important


## Continuous modeling approaches
We formulate a model generally as: $Y=f(X_1,X_2,...)+e$

If the outcome Y is continuous, these are some common models:

* Simple Linear model: $Y=b_0 + b_1X_1 + b_2X_2 + ...$
* Linear model with higher orders: $Y=b_0 + b_1X_1 + b_2X_2 + b_3 X_1^2 + b_4 X_2^2 + b_5 X_1X_2 + ...$

## Some comments on modeling approaches

* In statistics, linear refers to the parameters/coefficients. So the previous models are linear
models for statisticians (but not for engineers or physicists)
* If you include higher order terms, the base terms **also need to be in the model**.
* Models with higher order terms ($X^2$, $X^3$, ...) are often called polynomial models
* The interpretation of higher order coefficients is less intuitive.

## Continuous modeling approaches
Many other models can be used for a continuous outcome:

* Models that include piecewise polynomials or splines
* Generalized additive models: $Y=b_0 + f_1(X_1) + f_2(X_2) + ...$
* Regression trees and forests
* Support vector machines
* Neural Nets
* ...

For some of those models, writing down the equation that connects Y with the $X_i$ is often hard or impossible. Instead, the model is described by an algorithm.

## Categorical modeling approaches
We formulate a model generally as: $g(Y)=f(X_1,X_2,...)+e$

If the outcome Y is categorical, these are some common models:

* A simple logistic model: $\log \left( \frac{Y}{1-Y} \right) = b_0 + b_1X_1 + b_2 X_2 + ...$ (Y is now a probability)
* More complicated logistic model: $\log \left( \frac{Y}{1-Y} \right) = b_0 + b_1X_1 + b_1 X_1^2 + f_2(X_2) + ...$ 

**We can do anything with the predictors we did for continuous outcomes**


## Categorical modeling approaches
Many other models can be used for categorical outcomes:

* Linear and Quadratic Discriminant Analysis
* Regression trees and forests
* Support vector machines
* ...

Again, for some of these models, we can't formulate an equation, but we can describe the algorithm.


## Summary
* There are many different modeling approaches
* Most of them are relatively easy to implement in R
* **It's hard to pick the right one! More on that next...**





* Chapters 1 and 2 of _Applied Predictive Modeling_ by Kuhn (in _general resources_ folder). 
* Chapter 2.1 of _An Introduction to Statistical Learning_ by James et al. (in _general resources_ folder). I recommend also reading the introduction, chapter 1. 

## Optional
* [R for Data Science](http://r4ds.had.co.nz/index.html), chapters 22-24.




# Topical Cartoon
![Maybe Dogbert didn't do the analysis quite right](../media/dilbert_datamining.gif)



---
title: MADA Fall 2019 - Model Fitting Overview
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r Sys.Date()`"
#bibliography: ../media/references.bib
output: 
  html_document:
    toc_depth: 3
---

```{r, include=FALSE}
#just so I can use emoticons
#devtools::install_github("hadley/emo")
library(emo)
#set some figure options
knitr::opts_chunk$set(out.width = '70%')
```


# Overview
In this module, we will talk about the whole data analysis workflow in a general, big-picture sense before we go into some of the details in further modules.


# Learning Objectives
*	Understand the elements of a data analysis  
*	Be able to set up a structure for an efficient, reproducible analysis



# Data Analysis Types
No matter what analysis you do, you will need to do the steps of getting and cleaning/processing/wrangling the data. You also always want to explore your data. After that, the further analyis steps you do depend on your goals.

The simplest analysis is a **descriptive** one. At that stage, you process, summarize and present the data, and do not go further. Here, you don't need to fit any statistical models.

* **Analytical/Inferential:** "Statistical Probing" of patterns. e.g. Hypothesis is that there is a negative correlation between age and measles incidence, or that there is a difference between males and females. We can use the right data and model to test if that's true and how strong the correlation is.
* **Predictive:** Given all kinds of information (e.g. gender, age), try to predict future risk of measles infection.
* **Causal:** Trying to show that ice cream causes measles. For that, data needs to be collected in the right way.
* **Mechanistic:** Build a model that explicitly includes mechanisms of measles infection, e.g. a process by which an infected person transmits to a non-infected person. 


### Needs model fitting:
* **Analytical/Inferential: Quantitatively understand relations between inputs/predictors/features and outputs/outcomes**
    * Causal interpretation possible under certain circumstances
* **Predictive: Use information on inputs to predict outputs**
* Mechanistic: Can be used with the right kind of data and models, both in an inferential and predictive way

## Why fit models
* To test hypotheses (e.g. "there is a (linear) _correlation_ between BMI and diabetes")
* To estimate parameters (e.g. "a 1 unit increase of particulate matter leads to 2 more asthma attacks per person per year")
* To make predictions (e.g. " exposure to N cholera bacteria leads to an infection with probability p") 
* To draw causal conclusions (e.g. "taking statins _causes_ reduction in cholesterol")
* To draw mechanistic conclusions (e.g. "interferon reduces HCV virus load _by stopping production of new virus_") 

## Inference as goal
* Focus is on understanding the relation between inputs/predictors/features and outputs/outcome
* Simpler models (e.g. linear/logistic regression, GLM, Classification and Regression Trees) are usually better, allow for easier interpretation of results
* Examples: 
    * Is the relation between some chemical and cancer risk linear, and how much would risk increase if the chemical exposure increased by 1 unit?
    * Understanding which markers (e.g. cytokines in blood) are most influential for a given clinical outcome

## Prediction as goal
* Focus is on predicting (new) output/outcomes based on knowledge of inputs/predictors/features
* Understanding relation between input(s) and output(s) not that important
* High performance, "black box" models (e.g. GAM, Forests, SVM, Neural Nets) are often used.
* Examples: 
    * Netflix/Amazon recommendations based on past behavior
    * Prediction of clinical outcome based on a large set of markers (e.g. cytokines in blood)


## Types of Analyses based on outcome
* If an outcome is known/available, the statistical methods employed are usually called supervised learning methods/algorithms.
    * Example: Look for associations (make predictions) between flu patient characteristics and occurrence of hospitalization.
    
* If an outcome is unknown/unavailable, the statistical methods are called unsupervised learning methods/algorithms.
    * Example: Try to group pictures of cells into categories of normal/abnormal, without knowing for any of the pictures if they are normal or not.

_(Almost) all models you have encountered so far in your work are likely supervised methods._

## Types of Analyses based on outcome

### Outcome is a continuous variable (or can be treated as one)
Regression methods: (generalized) linear models, generalized additive models, trees, support vector machines, neural nets, ...

Examples: Cholesterol level, number of infected cases during outbreak

### Outcome is categorical
Classification Methods: Logistic regression, K-nearest neighbors, Linear Discriminant Analysis, GAM, trees, SVM, NN,...

Examples: Yes/No outcome, low/medium/high responders

## Types of Analyses based on outcome

### Outcome is unknown/does not exist
Clustering methods: K-means clustering, Principal Component Analysis (also used for other purposes), ...

Example: Group gene sequences based on sequence similarity


## The _no free lunch_ theorem
http://en.wikipedia.org/wiki/No_free_lunch_theorem

* There is no method that is universally best for all data/questions
* Even for a given dataset, there are always trade-offs between methods/approaches 

## Machine learning methods
```{r islrfig,  fig.show='hold', out.width = "600px", fig.cap='From: "An Introduction to Statistical Learning with Applications in R" (ISLR) by James et al.', echo=FALSE}
knitr::include_graphics("../media/islr-fig27.png")
```


## Supervised modeling overview
* We consider models where we have a single outcome, Y
* Y can be continuous or categorical
* We usually have multiple predictors, $X_1, X_2,...$
* Sometimes, one of the predictors is our main focus (exposure), and others are covariates
* Sometimes, all predictors are considered equally important


## Continuous modeling approaches
We formulate a model generally as: $Y=f(X_1,X_2,...)+e$

If the outcome Y is continuous, these are some common models:

* Simple Linear model: $Y=b_0 + b_1X_1 + b_2X_2 + ...$
* Linear model with higher orders: $Y=b_0 + b_1X_1 + b_2X_2 + b_3 X_1^2 + b_4 X_2^2 + b_5 X_1X_2 + ...$

## Some comments on modeling approaches

* In statistics, linear refers to the parameters/coefficients. So the previous models are linear
models for statisticians (but not for engineers or physicists)
* If you include higher order terms, the base terms **also need to be in the model**.
* Models with higher order terms ($X^2$, $X^3$, ...) are often called polynomial models
* The interpretation of higher order coefficients is less intuitive.

## Continuous modeling approaches
Many other models can be used for a continuous outcome:

* Models that include piecewise polynomials or splines
* Generalized additive models: $Y=b_0 + f_1(X_1) + f_2(X_2) + ...$
* Regression trees and forests
* Support vector machines
* Neural Nets
* ...

For some of those models, writing down the equation that connects Y with the $X_i$ is often hard or impossible. Instead, the model is described by an algorithm.

## Categorical modeling approaches
We formulate a model generally as: $g(Y)=f(X_1,X_2,...)+e$

If the outcome Y is categorical, these are some common models:

* A simple logistic model: $\log \left( \frac{Y}{1-Y} \right) = b_0 + b_1X_1 + b_2 X_2 + ...$ (Y is now a probability)
* More complicated logistic model: $\log \left( \frac{Y}{1-Y} \right) = b_0 + b_1X_1 + b_1 X_1^2 + f_2(X_2) + ...$ 

**We can do anything with the predictors we did for continuous outcomes**


## Categorical modeling approaches
Many other models can be used for categorical outcomes:

* Linear and Quadratic Discriminant Analysis
* Regression trees and forests
* Support vector machines
* ...

Again, for some of these models, we can't formulate an equation, but we can describe the algorithm.


## Summary
* There are many different modeling approaches
* Most of them are relatively easy to implement in R
* **It's hard to pick the right one! More on that next...**





* Chapters 1 and 2 of _Applied Predictive Modeling_ by Kuhn (in _general resources_ folder). 
* Chapter 2.1 of _An Introduction to Statistical Learning_ by James et al. (in _general resources_ folder). I recommend also reading the introduction, chapter 1. 

## Optional
* [R for Data Science](http://r4ds.had.co.nz/index.html), chapters 22-24.




# Topical Cartoon
![Maybe Dogbert didn't do the analysis quite right](../media/dilbert_datamining.gif)


